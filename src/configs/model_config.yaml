# Training parameters
random_seed: 42
valid_ratio: 0.2
batch_size: 64
n_epochs: 1
early_stopping_patience: 2

# Model parameters
model_params:
  n_factors: 80
  learning_rate: 0.0005
  dropout: 0.3
  weight_decay: 0.01
  max_grad_norm: 1.0
  device: "cuda"  # or "cpu"

# LLM parameters
llm_params:
  provider: "sentence_transformers"
  model_name: "all-MiniLM-L6-v2"
  embedding_dim: 384
  use_cached_embeddings: true
  
# Evaluation parameters
metrics:
  use_ndcg: true
  use_map: true
  top_k: [5, 10, 20]
  
# Logging parameters
logging:
  level: "INFO"
  save_model_history: true
  checkpoint_frequency: 5  # epochs